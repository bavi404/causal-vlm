# Causal VLM: Audio-Visual Causal Analysis

A comprehensive framework for evaluating causal relationships in audio-visual multimodal models using ImageBind.

## Project Structure

```
causal-vlm/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ imagebind/          # Meta's ImageBind (cloned)
â”‚   â”‚   â””â”€â”€ imagebind_wrapper.py # Wrapper for easy embedding extraction
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â”œâ”€â”€ music_avqa.py       # Music-AVQA dataset loader
â”‚   â”‚   â”œâ”€â”€ avqa.py             # AVQA dataset loader
â”‚   â”‚   â””â”€â”€ audiocaps.py        # AudioCaps dataset loader
â”‚   â”œâ”€â”€ evaluations/
â”‚   â”‚   â”œâ”€â”€ baseline_eval.py    # Retrieval & QA evaluation
â”‚   â”‚   â””â”€â”€ make_results_table.py # Results aggregation
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ interventions.py   # Causal interventions (mask, swap)
â”‚   â”‚   â””â”€â”€ fusion.py           # Fusion strategies (early, late, multimodal)
â”‚   â””â”€â”€ run_baselines.py        # Main evaluation script
â”œâ”€â”€ tests/                       # Comprehensive test suite
â”œâ”€â”€ run_all.sh                  # Run all experiments (Bash)
â”œâ”€â”€ run_all.ps1                 # Run all experiments (PowerShell)
â””â”€â”€ requirements.txt
```

## Quick Start

### 1. Installation

```bash
pip install -r requirements.txt
```

### 2. Prepare Data

Organize your datasets:
```
data/
â”œâ”€â”€ music-avqa/
â”‚   â”œâ”€â”€ annotations.json
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ audio/
â”œâ”€â”€ avqa/
â”‚   â”œâ”€â”€ annotations.csv
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ audio/
â””â”€â”€ audiocaps/
    â”œâ”€â”€ annotations.json
    â”œâ”€â”€ images/
    â””â”€â”€ audio/
```

### 3. Run Experiments

**Run all experiments:**
```bash
# Linux/Mac/Git Bash
bash run_all.sh

# Windows PowerShell
.\run_all.ps1
```

**Run single experiment:**
```bash
python src/run_baselines.py \
    --dataset music-avqa \
    --fusion early \
    --intervention present \
    --annotations data/music-avqa/annotations.json \
    --data-root data/music-avqa
```

### 4. Generate Results Table

```bash
python src/evaluations/make_results_table.py \
    --results-dir results \
    --output-dir results/tables
```

## Key Features

### ðŸ”¬ Causal Interventions
- **Audio Present**: Baseline (no intervention)
- **Audio Masked**: Zero out audio embeddings
- **Audio Swapped**: Swap audio embeddings between samples

### ðŸ”€ Fusion Strategies
- **Early Fusion**: Mean of image + audio embeddings
- **Late Fusion**: Concatenation of embeddings
- **Multimodal Fusion**: Transformer-based fusion

### ðŸ“Š Evaluation Metrics
- **Retrieval**: R@1, R@5, R@10
- **QA**: Accuracy, mean similarity

### ðŸ“ˆ Results
- Automatic CSV logging
- Aggregated results tables (Markdown, CSV, PNG)
- Cached embeddings for fast re-evaluation

## Experiments

The framework runs **27 experiments** (3 datasets Ã— 3 interventions Ã— 3 fusion types):

| Dataset | Intervention | Fusion |
|---------|--------------|--------|
| MUSIC-AVQA | present/masked/swapped | early/late/multimodal |
| AVQA | present/masked/swapped | early/late/multimodal |
| Audiocaps | present/masked/swapped | early/late/multimodal |

## Results Location

- **Individual Results**: `results/{dataset}/{fusion}_{intervention}.csv`
- **Aggregated Table**: `results/tables/results_table.{md,csv,png}`
- **Cached Embeddings**: `cache/embeddings/{dataset}/`

## Testing

Run the test suite:
```bash
pytest tests/ -v
```

Tests cover:
- âœ… Audio masking produces all zeros
- âœ… Audio swapping matches source
- âœ… Fusion shapes are correct
- âœ… Retrieval R@1 increases with audio present vs masked

## Citation

If you use this framework, please cite:
- ImageBind: [Paper](https://arxiv.org/abs/2305.05665)
- This framework (if applicable)

## License

See individual component licenses (ImageBind uses CC-BY-NC 4.0).


